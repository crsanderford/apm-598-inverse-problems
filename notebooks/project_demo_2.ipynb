{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "import scipy\n",
    "\n",
    "\n",
    "from utils import *\n",
    "from pytorch_sparse_utils import *\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('Using {} device'.format(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "bead_reads = pd.read_csv('/data/bioprotean/macosko_et_al/Puck_200115_08_digital_expression_sorted.csv', index_col='Unnamed: 0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0610005C13Rik</th>\n",
       "      <th>0610007P14Rik</th>\n",
       "      <th>0610009B22Rik</th>\n",
       "      <th>0610009E02Rik</th>\n",
       "      <th>0610009L18Rik</th>\n",
       "      <th>0610009O20Rik</th>\n",
       "      <th>0610010F05Rik</th>\n",
       "      <th>0610010K14Rik</th>\n",
       "      <th>0610011F06Rik</th>\n",
       "      <th>0610012D04Rik</th>\n",
       "      <th>...</th>\n",
       "      <th>mt-Tq</th>\n",
       "      <th>mt-Tr</th>\n",
       "      <th>mt-Ts2</th>\n",
       "      <th>mt-Tt</th>\n",
       "      <th>mt-Tv</th>\n",
       "      <th>n-R5-8s1</th>\n",
       "      <th>n-R5s173</th>\n",
       "      <th>n-R5s33</th>\n",
       "      <th>n-R5s40</th>\n",
       "      <th>n-R5s95</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AAAAAAAGGTAGTA</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AAAAAAAGTCCCAA</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AAAAAAATCTTAGT</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AAAAAACATCTTTC</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AAAAAACGAAATAG</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 23264 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                0610005C13Rik  0610007P14Rik  0610009B22Rik  0610009E02Rik  \\\n",
       "AAAAAAAGGTAGTA              0              0              0              0   \n",
       "AAAAAAAGTCCCAA              0              0              0              0   \n",
       "AAAAAAATCTTAGT              0              0              0              0   \n",
       "AAAAAACATCTTTC              0              0              0              0   \n",
       "AAAAAACGAAATAG              0              0              0              0   \n",
       "\n",
       "                0610009L18Rik  0610009O20Rik  0610010F05Rik  0610010K14Rik  \\\n",
       "AAAAAAAGGTAGTA              0              0              0              0   \n",
       "AAAAAAAGTCCCAA              0              0              0              0   \n",
       "AAAAAAATCTTAGT              0              0              0              0   \n",
       "AAAAAACATCTTTC              0              0              0              0   \n",
       "AAAAAACGAAATAG              0              0              0              0   \n",
       "\n",
       "                0610011F06Rik  0610012D04Rik  ...  mt-Tq  mt-Tr  mt-Ts2  \\\n",
       "AAAAAAAGGTAGTA              0              0  ...      0      0       0   \n",
       "AAAAAAAGTCCCAA              0              0  ...      0      0       0   \n",
       "AAAAAAATCTTAGT              0              0  ...      0      0       0   \n",
       "AAAAAACATCTTTC              0              0  ...      0      0       0   \n",
       "AAAAAACGAAATAG              0              0  ...      0      0       0   \n",
       "\n",
       "                mt-Tt  mt-Tv  n-R5-8s1  n-R5s173  n-R5s33  n-R5s40  n-R5s95  \n",
       "AAAAAAAGGTAGTA      0      0         0         0        0        0        0  \n",
       "AAAAAAAGTCCCAA      0      0         0         0        0        0        0  \n",
       "AAAAAAATCTTAGT      0      0         0         0        0        0        0  \n",
       "AAAAAACATCTTTC      0      0         0         0        0        0        0  \n",
       "AAAAAACGAAATAG      0      0         0         0        0        0        0  \n",
       "\n",
       "[5 rows x 23264 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bead_reads.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_scipy_sparse = scipy.sparse.coo_matrix( bead_reads.to_numpy() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sparse_tensor = scipy_sparse_to_pytorch_sparse(data_scipy_sparse).to(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(indices=tensor([[    0,     0,     0,  ..., 53207, 53207, 53207],\n",
       "                       [ 2667,  2717,  6164,  ..., 23252, 23253, 23255]]),\n",
       "       values=tensor([1., 1., 1.,  ..., 1., 1., 1.]),\n",
       "       size=(53208, 23264), nnz=22396657, layout=torch.sparse_coo)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_sparse_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dense_tensor = data_sparse_tensor.to_dense().to(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_scipy_sparse = (data_scipy_sparse != 0).astype(np.int64).tocoo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_sparse_tensor = scipy_sparse_to_pytorch_sparse(mask_scipy_sparse).to(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_dense_tensor = mask_sparse_tensor.to_dense().bool().to(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances = np.load('../data/macosko_distance_matrix.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "adjacency = scipy.sparse.coo_matrix( distances <= 20 , dtype=np.float32)\n",
    "degree = scipy.sparse.diags( adjacency.sum(axis=0).A1 )\n",
    "laplacian = (degree - adjacency).tocoo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "adjacency_sparse_tensor = scipy_sparse_to_pytorch_sparse(adjacency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "laplacian_sparse_tensor = scipy_sparse_to_pytorch_sparse(laplacian).to(device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "laplacian_tensor = torch.tensor(laplacian.todense()).to(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MatrixFactorization(torch.nn.Module):\n",
    "    def __init__(self, rows, cols, n_factors=10):\n",
    "        super().__init__()\n",
    "        self.row_embedding = torch.nn.parameter.Parameter(torch.rand(rows, n_factors), requires_grad=True)\n",
    "        self.col_embedding = torch.nn.parameter.Parameter(torch.rand(n_factors, cols), requires_grad=True)\n",
    "\n",
    "    def forward(self):\n",
    "        return torch.matmul( self.row_embedding, self.col_embedding )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def laplacian_loss(x, x_lap):\n",
    "    \n",
    "    lap_sum = 0\n",
    "    for ii in range(x.shape[1]):\n",
    "        lap_sum = lap_sum + bilinear_mult(x[:,ii], x_lap)\n",
    "        \n",
    "    return lap_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "mf = MatrixFactorization(mask_sparse_tensor.shape[0], mask_sparse_tensor.shape[1], 10).to(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(mf.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "\n",
    "for epoch in range(1000):\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    output = mf.forward()\n",
    "    loss = criterion(output[mask_dense_tensor], data_dense_tensor[mask_dense_tensor]) + 0.001*laplacian_loss(output, laplacian_sparse_tensor)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(epoch)\n",
    "        print(loss)\n",
    "        \n",
    "    losses.append(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'losses' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_184743/1001488902.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlosses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'losses' is not defined"
     ]
    }
   ],
   "source": [
    "losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11.324365056"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(adjacency_tensor.element_size() * adjacency_tensor.nelement())/(10**9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "22ee4ece52021b4a120b95b326dd7a4d7938576e919f53927a7959ab5d7e1e45"
  },
  "kernelspec": {
   "display_name": "spatialSFT",
   "language": "python",
   "name": "spatialsft"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
