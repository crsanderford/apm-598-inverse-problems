{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "import scipy\n",
    "\n",
    "\n",
    "from utils import *\n",
    "from pytorch_sparse_utils import *\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### set parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "DECOMP_RANK = 10\n",
    "LAPLACIAN_PARAM = 0.001\n",
    "TV_PARAM = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('Using {} device'.format(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1110008P14Rik</th>\n",
       "      <th>1500009C09Rik</th>\n",
       "      <th>1500012F01Rik</th>\n",
       "      <th>1700020I14Rik</th>\n",
       "      <th>2010107E04Rik</th>\n",
       "      <th>2010300C02Rik</th>\n",
       "      <th>2210016L21Rik</th>\n",
       "      <th>2310036O22Rik</th>\n",
       "      <th>2900011O08Rik</th>\n",
       "      <th>3110035E14Rik</th>\n",
       "      <th>...</th>\n",
       "      <th>Zrsr1</th>\n",
       "      <th>Zwint</th>\n",
       "      <th>mt-Co1</th>\n",
       "      <th>mt-Cytb</th>\n",
       "      <th>mt-Nd1</th>\n",
       "      <th>mt-Nd2</th>\n",
       "      <th>mt-Nd4</th>\n",
       "      <th>mt-Nd5</th>\n",
       "      <th>mt-Rnr1</th>\n",
       "      <th>mt-Rnr2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AAAAAAAGGTAGTA</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AAAAAAAGTCCCAA</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AAAAAAATCTTAGT</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AAAAAACATCTTTC</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AAAAAACGAAATAG</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 1222 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                1110008P14Rik  1500009C09Rik  1500012F01Rik  1700020I14Rik  \\\n",
       "AAAAAAAGGTAGTA              0              0              0              0   \n",
       "AAAAAAAGTCCCAA              0              0              0              0   \n",
       "AAAAAAATCTTAGT              0              0              0              0   \n",
       "AAAAAACATCTTTC              0              0              0              0   \n",
       "AAAAAACGAAATAG              0              0              0              0   \n",
       "\n",
       "                2010107E04Rik  2010300C02Rik  2210016L21Rik  2310036O22Rik  \\\n",
       "AAAAAAAGGTAGTA              0              0              0              0   \n",
       "AAAAAAAGTCCCAA              0              0              0              0   \n",
       "AAAAAAATCTTAGT              0              0              0              0   \n",
       "AAAAAACATCTTTC              0              0              0              0   \n",
       "AAAAAACGAAATAG              0              0              0              0   \n",
       "\n",
       "                2900011O08Rik  3110035E14Rik  ...  Zrsr1  Zwint  mt-Co1  \\\n",
       "AAAAAAAGGTAGTA              0              0  ...      0      0       0   \n",
       "AAAAAAAGTCCCAA              0              0  ...      0      0       2   \n",
       "AAAAAAATCTTAGT              0              0  ...      0      0       0   \n",
       "AAAAAACATCTTTC              0              0  ...      0      0       0   \n",
       "AAAAAACGAAATAG              0              0  ...      0      0       1   \n",
       "\n",
       "                mt-Cytb  mt-Nd1  mt-Nd2  mt-Nd4  mt-Nd5  mt-Rnr1  mt-Rnr2  \n",
       "AAAAAAAGGTAGTA        0       0       0       0       0        1        0  \n",
       "AAAAAAAGTCCCAA        2       5       0       1       0        2        4  \n",
       "AAAAAAATCTTAGT        0       0       0       0       0        0        0  \n",
       "AAAAAACATCTTTC        0       0       0       0       0        1        0  \n",
       "AAAAAACGAAATAG        0       0       0       0       0        0        0  \n",
       "\n",
       "[5 rows x 1222 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bead_reads_subset = pd.read_csv('./data/bead_reads_subset.csv', index_col='Unnamed: 0')\n",
    "bead_reads_subset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_scipy_sparse = scipy.sparse.coo_matrix( bead_reads_subset.to_numpy() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sparse_tensor = scipy_sparse_to_pytorch_sparse(data_scipy_sparse).to(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(indices=tensor([[    0,     0,     0,  ..., 53207, 53207, 53207],\n",
       "                       [   84,   412,   528,  ...,  1219,  1220,  1221]]),\n",
       "       values=tensor([ 1.,  1.,  1.,  ..., 12., 26., 71.]),\n",
       "       device='cuda:0', size=(53208, 1222), nnz=9552845, layout=torch.sparse_coo)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_sparse_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dense_tensor = data_sparse_tensor.to_dense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_scipy_sparse = (data_scipy_sparse != 0).astype(np.int64).tocoo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_sparse_tensor = scipy_sparse_to_pytorch_sparse(mask_scipy_sparse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_dense_tensor = mask_sparse_tensor.to_dense().bool()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances = np.load('./data/macosko_distance_matrix.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "adjacency = scipy.sparse.coo_matrix( distances <= 20 , dtype=np.float32)\n",
    "degree = scipy.sparse.diags( adjacency.sum(axis=0).A1 )\n",
    "laplacian = (degree - adjacency).tocoo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "adjacency_sparse_tensor = scipy_sparse_to_pytorch_sparse(adjacency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "adjacency_dense_tensor = adjacency_sparse_tensor.to_dense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def laplacian_loss(x, x_lap):\n",
    "    \n",
    "    lap_sum = 0\n",
    "    for ii in range(x.shape[1]):\n",
    "        lap_sum = lap_sum + (x[:,ii].T @ x_lap @ x[:,ii])\n",
    "        \n",
    "    return lap_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "mf = MatrixFactorization(mask_sparse_tensor.shape[0], mask_sparse_tensor.shape[1], DECOMP_RANK).to(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.Adam(mf.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "215938957.0\n"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "\n",
    "for epoch in range(50):\n",
    "    \n",
    "    batcher = data_adjacency_batcher(data_dense_tensor, adjacency_dense_tensor, 5000)\n",
    "    \n",
    "    total_loss = 0\n",
    "    \n",
    "    \n",
    "    for (data_batch, mask_batch, adjacency_batch, row_batch) in batcher:\n",
    "        \n",
    "        \n",
    "        \n",
    "        data_batch = data_batch.to(device=device)\n",
    "        mask_batch = mask_batch.to(device=device)\n",
    "        adjacency_batch = adjacency_batch.to(device=device)\n",
    "        row_batch = row_batch.to(device=device)\n",
    "        \n",
    "        laplacian_tensor = adjacency_to_laplacian(adjacency_batch).to(device=device)\n",
    "    \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output = mf.forward(row_batch, torch.arange(0, data_batch.shape[1]))\n",
    "        \n",
    "        L1_laplacian_tensor = adjacency_to_laplacian(adjacency_to_L1_adjacency(output, adjacency_batch, 0.001, device)).to(device=device)\n",
    "        \n",
    "        loss = squared_loss(data_batch[mask_batch], output[mask_batch]) + LAPLACIAN_PARAM*laplacian_loss(output, laplacian_tensor) + TV_PARAM*laplacian_loss(output, L1_laplacian_tensor)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "\n",
    "    if epoch % 1 == 0:\n",
    "        print(epoch)\n",
    "        print(total_loss)\n",
    "        \n",
    "    losses.append(total_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11001305.0"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(f'./results/loss_curves/mf_rank{DECOMP_RANK}_L{LAPLACIAN_PARAM}_TV{TV_PARAM}_losses', np.array(losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(mf.state_dict(), f'./results/saved_models/mf_rank{DECOMP_RANK}_L{LAPLACIAN_PARAM}_TV{TV_PARAM}_state')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_output = mf.forward(torch.arange(0, data_dense_tensor.shape[0]), torch.arange(0, data_dense_tensor.shape[1]))\n",
    "\n",
    "final_squared_error = squared_loss(data_dense_tensor, final_output)\n",
    "final_laplacian = laplacian_loss(final_output, adjacency_to_laplacian(adjacency_dense_tensor).to(device=device))\n",
    "final_TV =  laplacian_loss(final_output, adjacency_to_laplacian(adjacency_to_L1_adjacency(final_output, adjacency_dense_tensor, 0.001, 'cpu')).to(device=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(f'./results/final_losses/mf_rank{DECOMP_RANK}_L{LAPLACIAN_PARAM}_TV{TV_PARAM}_losses', np.array([final_squared_error.item(), final_laplacian.item(), final_TV.item()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "22ee4ece52021b4a120b95b326dd7a4d7938576e919f53927a7959ab5d7e1e45"
  },
  "kernelspec": {
   "display_name": "spatialSFT",
   "language": "python",
   "name": "spatialsft"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
